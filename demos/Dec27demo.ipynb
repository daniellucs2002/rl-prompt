{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dec 27 2023 work\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"]=\"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Questions:\n",
    "\n",
    "1. the generation of prompts appears to be very random, which might not be applicable to code snippets\n",
    "\n",
    "2. CodeT5 performance under some traditional code obfuscation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a demo of _get_generation_cache function\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # <|endoftext|> (id:50256)\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _get_generation_cache(self, source_texts: List[str], past_key_values=None):\n",
    "        # [\"Hello, world!\", \"How are you doing?\"]\n",
    "        token_encoding = (self.tokenizer(source_texts,\n",
    "                                         padding=True,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt')\n",
    "                          .to(self.device))\n",
    "        # {'input_ids': tensor([[15496,11,995,0,50256],[2437,389,345,1804,30]], device='cuda:1'),\n",
    "        #  'attention_mask': tensor([[1,1,1,1,0],[1,1,1,1,1]], device='cuda:1')}\n",
    "        input_ids = token_encoding['input_ids']  # 2*5, note that there's a padding 50256\n",
    "        input_lengths = token_encoding['attention_mask'].sum(dim=1)  # sum up horizontally\n",
    "        outputs = self.model.transformer(input_ids,\n",
    "                                         past_key_values=past_key_values,  # intermediate outputs saved\n",
    "                                         use_cache=True)\n",
    "        # Fields: last_hidden_state (torch.Size([2, 5, 768])) past_key_values (<class 'tuple'>)\n",
    "        last_token_hidden_state = \\\n",
    "            outputs.last_hidden_state[np.arange(input_ids.shape[0]),\n",
    "                                      (input_lengths - 1)]  # torch.Size([2, 768])\n",
    "        past_key_values = outputs.past_key_values\n",
    "        return last_token_hidden_state, past_key_values\n",
    "\n",
    "    def main(self):\n",
    "        source_texts = [\"Hello, world!\", \"How are you doing?\"]\n",
    "        self._get_generation_cache(source_texts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lm = LanguageModel()\n",
    "    lm.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a demo of text generation process\n",
    "\n",
    "class GenerationModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _get_generation_cache(self, source_texts: List[str], past_key_values=None):\n",
    "        # [\"Hello, world!\", \"How are you doing?\"]\n",
    "        token_encoding = (self.tokenizer(source_texts,\n",
    "                                         padding=True,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt')\n",
    "                          .to(self.device))\n",
    "        # {'input_ids': tensor([[15496,11,995,0,50256],[2437,389,345,1804,30]], device='cuda:1'),\n",
    "        #  'attention_mask': tensor([[1,1,1,1,0],[1,1,1,1,1]], device='cuda:1')}\n",
    "        input_ids = token_encoding['input_ids']  # 2*5, note that there's a padding 50256\n",
    "        input_lengths = token_encoding['attention_mask'].sum(dim=1)  # sum up horizontally\n",
    "        outputs = self.model.transformer(input_ids,\n",
    "                                         past_key_values=past_key_values,  # intermediate outputs saved\n",
    "                                         use_cache=True)\n",
    "        # Fields: last_hidden_state (torch.Size([2, 5, 768])) past_key_values (<class 'tuple'>)\n",
    "        last_token_hidden_state = \\\n",
    "            outputs.last_hidden_state[np.arange(input_ids.shape[0]),\n",
    "                                      (input_lengths - 1)]  # torch.Size([2, 768])\n",
    "        past_key_values = outputs.past_key_values\n",
    "        return last_token_hidden_state, past_key_values\n",
    "\n",
    "    def generate_text(self, source_texts, max_new_tokens=5):\n",
    "        generated_texts = source_texts.copy()  # Copy the source texts\n",
    "        past_key_values = None  # Initialize past_key_values\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Process all texts in the batch\n",
    "            state, past_key_values = self._get_generation_cache(generated_texts, past_key_values)\n",
    "\n",
    "            # Generate next tokens for each text in the batch\n",
    "            next_tokens = []\n",
    "            # state.shape: torch.Size([2, 768])\n",
    "            for idx, state_per_text in enumerate(state):\n",
    "                # state_per_text.shape: torch.Size([768])\n",
    "                logits = state_per_text.unsqueeze(0)  # Add batch dimension\n",
    "                # logits.shape: torch.Size([1, 768])\n",
    "                # Pass the logits through the LM head to get predictions for the entire vocabulary\n",
    "                logits = self.model.lm_head(logits)  # torch.Size([1, 50257])\n",
    "                next_token_id = torch.argmax(logits, dim=-1)\n",
    "                next_token = self.tokenizer.decode(next_token_id)\n",
    "                next_tokens.append(next_token)\n",
    "\n",
    "            # Append the generated tokens to the respective texts\n",
    "            generated_texts = [text + token for text, token in zip(generated_texts, next_tokens)]\n",
    "\n",
    "        return generated_texts\n",
    "\n",
    "    def main(self):\n",
    "        source_texts = [\"Hello, Daniel Lu!\", \"I love computer science so much...\"]\n",
    "        generated_texts = self.generate_text(source_texts, max_new_tokens=5)\n",
    "        for text in generated_texts:\n",
    "            print(text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lm = GenerationModel()\n",
    "    lm.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a demo of top-k sampling approach\n",
    "\n",
    "class SamplingModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _get_generation_cache(self, source_texts: List[str], past_key_values=None):\n",
    "        # [\"Hello, world!\", \"How are you doing?\"]\n",
    "        token_encoding = (self.tokenizer(source_texts,\n",
    "                                         padding=True,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt')\n",
    "                          .to(self.device))\n",
    "        # {'input_ids': tensor([[15496,11,995,0,50256],[2437,389,345,1804,30]], device='cuda:1'),\n",
    "        #  'attention_mask': tensor([[1,1,1,1,0],[1,1,1,1,1]], device='cuda:1')}\n",
    "        input_ids = token_encoding['input_ids']  # 2*5, note that there's a padding 50256\n",
    "        input_lengths = token_encoding['attention_mask'].sum(dim=1)  # sum up horizontally\n",
    "        outputs = self.model.transformer(input_ids,\n",
    "                                         past_key_values=past_key_values,  # intermediate outputs saved\n",
    "                                         use_cache=True)\n",
    "        # Fields: last_hidden_state (torch.Size([2, 5, 768])) past_key_values (<class 'tuple'>)\n",
    "        last_token_hidden_state = \\\n",
    "            outputs.last_hidden_state[np.arange(input_ids.shape[0]),\n",
    "                                      (input_lengths - 1)]  # torch.Size([2, 768])\n",
    "        past_key_values = outputs.past_key_values\n",
    "        return last_token_hidden_state, past_key_values\n",
    "\n",
    "    def top_k_logits(self, logits, k):\n",
    "        if k == 0:\n",
    "            return logits  # Keep all logits\n",
    "        else:\n",
    "            # Remove all logits not in the top k\n",
    "            values, _ = torch.topk(logits, k)\n",
    "            min_values = values[:, -1].unsqueeze(1).expand_as(logits)\n",
    "            return torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "    def generate_text(self, source_texts, max_new_tokens=5, top_k=50):\n",
    "        generated_texts = source_texts.copy()  # Copy the source texts\n",
    "        past_key_values = None  # Initialize past_key_values\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Process all texts in the batch\n",
    "            state, past_key_values = self._get_generation_cache(generated_texts, past_key_values)\n",
    "\n",
    "            # Generate next tokens for each text in the batch\n",
    "            next_tokens = []\n",
    "            for idx, state_per_text in enumerate(state):\n",
    "                logits = state_per_text.unsqueeze(0)  # Add batch dimension\n",
    "                logits = self.model.lm_head(logits)  # Get logits for the entire vocabulary\n",
    "\n",
    "                # Apply top-k filtering\n",
    "                filtered_logits = self.top_k_logits(logits, top_k)\n",
    "\n",
    "                # Sample from the filtered distribution\n",
    "                probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)  # torch.Size([1, 50257])\n",
    "                # the likelihood of each element being selected is proportional to its probability in the distribution\n",
    "                next_token_id = torch.multinomial(probabilities, 1).item()\n",
    "                next_token = self.tokenizer.decode(next_token_id)\n",
    "\n",
    "                next_tokens.append(next_token)\n",
    "\n",
    "            # Append the generated tokens to the respective texts\n",
    "            generated_texts = [text + token for text, token in zip(generated_texts, next_tokens)]\n",
    "\n",
    "        return generated_texts\n",
    "\n",
    "    def main(self):\n",
    "        source_texts = [\"Hello, Daniel Lu!\", \"I love computer science so much...\"]\n",
    "        generated_texts = self.generate_text(source_texts, max_new_tokens=5)\n",
    "        for text in generated_texts:\n",
    "            print(text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lm = SamplingModel()\n",
    "    lm.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "1. do more CodeT5 experiments on name/data/flow obfuscation -> find flaws\n",
    "\n",
    "2. null prompt discussed -> focus on finding better variable names\n",
    "\n",
    "3. does 'space'/'return' matter -> more dramatic changes\n",
    "\n",
    "4. do more literature investigation -> how to process codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
